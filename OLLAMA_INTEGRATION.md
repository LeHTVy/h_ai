# ğŸ”— TÃ­ch há»£p Ollama AI vá»›i H-AI

H-AI há»— trá»£ tÃ­ch há»£p Ollama Ä‘á»ƒ sá»­ dá»¥ng AI local cho phÃ¢n tÃ­ch thÃ´ng minh vÃ  quyáº¿t Ä‘á»‹nh tá»± Ä‘á»™ng.

## ğŸ“‹ YÃªu cáº§u

1. **CÃ i Ä‘áº·t Ollama**
   ```bash
   # Ubuntu/Debian
   curl -fsSL https://ollama.ai/install.sh | sh
   
   # hoáº·c download tá»« https://ollama.ai/download
   ```

2. **Táº£i mÃ´ hÃ¬nh LLM**
   ```bash
   # VÃ­ dá»¥ vá»›i llama2 (mÃ´ hÃ¬nh máº·c Ä‘á»‹nh)
   ollama pull llama2
   
   # Hoáº·c cÃ¡c mÃ´ hÃ¬nh khÃ¡c:
   ollama pull mistral      # Mistral 7B - tá»‘t cho cybersecurity
   ollama pull codellama    # CodeLlama - tá»‘t cho phÃ¢n tÃ­ch code/exploit
   ollama pull deepseek-chat # DeepSeek - tá»‘t cho tiáº¿ng Viá»‡t
   ```

3. **Khá»Ÿi Ä‘á»™ng Ollama**
   ```bash
   # Ollama tá»± Ä‘á»™ng cháº¡y nhÆ° má»™t service
   # Kiá»ƒm tra xem Ä‘Ã£ cháº¡y chÆ°a:
   curl http://localhost:11434/api/tags
   ```

## ğŸš€ Sá»­ dá»¥ng

### 1. Khá»Ÿi Ä‘á»™ng H-AI vá»›i Ollama

```bash
# Sá»­ dá»¥ng mÃ´ hÃ¬nh máº·c Ä‘á»‹nh (llama2) vÃ  URL máº·c Ä‘á»‹nh (localhost:11434)
./bin/h-ai-server --ollama-model llama2

# Chá»‰ Ä‘á»‹nh URL Ollama tÃ¹y chá»‰nh
./bin/h-ai-server --ollama-url http://localhost:11434 --ollama-model mistral

# Hoáº·c táº¯t AI (chá»‰ dÃ¹ng rule-based logic)
./bin/h-ai-server
```

### 2. Kiá»ƒm tra tÃ­ch há»£p

```bash
# Health check sáº½ hiá»ƒn thá»‹ tráº¡ng thÃ¡i Ollama
curl http://localhost:8888/health

# Response sáº½ cÃ³:
# {
#   "ollama_enabled": true,
#   "ollama_model": "llama2"
# }
```

## ğŸ¯ Chá»©c nÄƒng AI

### 1. PhÃ¢n tÃ­ch Target thÃ´ng minh

```bash
curl -X POST http://localhost:8888/api/intelligence/analyze-target \
  -H "Content-Type: application/json" \
  -d '{"target": "example.com"}'
```

AI sáº½ phÃ¢n tÃ­ch target vÃ  Ä‘Æ°a ra:
- ÄÃ¡nh giÃ¡ rá»§i ro
- Má»©c Ä‘á»™ tin cáº­y
- CÃ¡c khuyáº¿n nghá»‹

### 2. Äá» xuáº¥t Tools tá»‘i Æ°u

```bash
curl -X POST http://localhost:8888/api/intelligence/select-tools \
  -H "Content-Type: application/json" \
  -d '{
    "target": "https://example.com",
    "target_type": "comprehensive"
  }'
```

AI sáº½ Ä‘á» xuáº¥t cÃ¡c tools phÃ¹ há»£p dá»±a trÃªn:
- Loáº¡i target (web app, network host, API)
- Technologies Ä‘Æ°á»£c phÃ¡t hiá»‡n
- Objective (quick, stealth, comprehensive)

### 3. Tá»‘i Æ°u hÃ³a Parameters

```bash
curl -X POST http://localhost:8888/api/intelligence/optimize-parameters \
  -H "Content-Type: application/json" \
  -d '{
    "tool": "nmap",
    "target": "example.com",
    "context": {"previous_scan": "ports 80,443 open"}
  }'
```

AI sáº½ tá»‘i Æ°u parameters dá»±a trÃªn:
- Context tá»« cÃ¡c scans trÆ°á»›c
- Target profile
- Best practices cho tool Ä‘Ã³

### 4. PhÃ¢n tÃ­ch Káº¿t quáº£ Scan

```bash
curl -X POST http://localhost:8888/api/intelligence/analyze-results \
  -H "Content-Type: application/json" \
  -d '{
    "tool": "nmap",
    "target": "example.com",
    "results": "PORT   STATE SERVICE\n80/tcp open  http\n443/tcp open https"
  }'
```

AI sáº½ phÃ¢n tÃ­ch vÃ  Ä‘Æ°a ra:
- Key findings vÃ  vulnerabilities tiá»m nÄƒng
- Recommended next steps
- Risk assessment
- Suggested follow-up tools

## ğŸ“Š Workflow vá»›i AI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Request    â”‚
â”‚ "Scan example"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Analyzes     â”‚ â† Ollama
â”‚ Target          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Suggests     â”‚ â† Ollama
â”‚ Tools & Params  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute Tools   â”‚
â”‚ (nmap, nuclei)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Analyzes     â”‚ â† Ollama
â”‚ Results         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Next Steps      â”‚
â”‚ Recommendations â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš™ï¸ Cáº¥u hÃ¬nh nÃ¢ng cao

### Environment Variables

```bash
export OLLAMA_URL=http://localhost:11434
export OLLAMA_MODEL=mistral
./bin/h-ai-server --ollama-url $OLLAMA_URL --ollama-model $OLLAMA_MODEL
```

### Chá»n Model phÃ¹ há»£p

| Model | Tá»‘t cho | KÃ­ch thÆ°á»›c | RAM cáº§n |
|-------|---------|------------|---------|
| llama2 | General purpose | 7B | 8GB |
| mistral | Cybersecurity | 7B | 8GB |
| codellama | Code analysis | 7B-13B | 8-16GB |
| deepseek-chat | Tiáº¿ng Viá»‡t | 7B | 8GB |
| llava | Image analysis | 7B | 8GB |

### Tá»‘i Æ°u Performance

1. **Sá»­ dá»¥ng GPU** (náº¿u cÃ³):
   ```bash
   # Ollama tá»± Ä‘á»™ng dÃ¹ng GPU náº¿u cÃ³ CUDA
   # Kiá»ƒm tra:
   ollama run llama2 "test"
   ```

2. **Giáº£m Temperature** cho káº¿t quáº£ nháº¥t quÃ¡n hÆ¡n:
   - Code Ä‘Ã£ set `Temperature: 0.3` cho analysis tasks
   - CÃ³ thá»ƒ Ä‘iá»u chá»‰nh trong `internal/ai/ollama.go`

3. **Cache responses**: H-AI tá»± Ä‘á»™ng cache káº¿t quáº£ Ä‘á»ƒ trÃ¡nh gá»i láº¡i

## ğŸ” Troubleshooting

### Ollama khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c

```bash
# Kiá»ƒm tra Ollama Ä‘ang cháº¡y
curl http://localhost:11434/api/tags

# Náº¿u khÃ´ng cÃ³ response, khá»Ÿi Ä‘á»™ng láº¡i:
ollama serve
```

### Model chÆ°a Ä‘Æ°á»£c táº£i

```bash
# Kiá»ƒm tra models Ä‘Ã£ táº£i:
ollama list

# Táº£i model:
ollama pull llama2
```

### Out of Memory

Náº¿u model quÃ¡ lá»›n, dÃ¹ng model nhá» hÆ¡n:
```bash
ollama pull llama2:7b  # Thay vÃ¬ 13b
```

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Available Models](https://ollama.ai/library)
- [API Reference](https://github.com/ollama/ollama/blob/main/docs/api.md)

## ğŸ’¡ Tips

1. **Test model trÆ°á»›c**: Cháº¡y `ollama run <model>` Ä‘á»ƒ Ä‘áº£m báº£o model hoáº¡t Ä‘á»™ng
2. **Monitor RAM**: LLMs tá»‘n nhiá»u RAM, Ä‘áº£m báº£o cÃ³ Ä‘á»§ tÃ i nguyÃªn
3. **Fallback tá»± Ä‘á»™ng**: Náº¿u Ollama khÃ´ng available, H-AI tá»± Ä‘á»™ng dÃ¹ng rule-based logic
4. **Káº¿t há»£p vá»›i MCP**: CÃ³ thá»ƒ dÃ¹ng cáº£ Ollama (local) vÃ  Claude Desktop (cloud) cÃ¹ng lÃºc

